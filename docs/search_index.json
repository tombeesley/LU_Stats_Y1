[["index.html", "Statistics for Psychologists 1 Intro", " Statistics for Psychologists John Towse, Tom Beesley, Margriet Groen, Rob Davies 2021-08-10 1 Intro This is a collection of tuition material written for Psychology undergraduates at Lancaster University. At the moment the content represents the lab materials for the PSYC121 and PSYC122 modules in first year. They feature tuition of programming with R, with material designed to be accessible for students without any programming background. The materials should be reasonably self-contained, though there are references to lecture content at times. Some of the lab content involves the use of learnr tutorials. These are online, interactive tutorials, that run in a browser. You should be able to access these by clicking on the relevant links. "],["introduction-to-r-studio-week-1.html", "2 Introduction to R Studio (Week 1) 2.1 Lab Work", " 2 Introduction to R Studio (Week 1) The exercises in this section are designed to familiarise you with working in R Studio. 2.1 Lab Work 2.1.1 Introducing RStudio R and RStudio is the software that we will be using to explore and learn about analysis in your Psychology degree. Its a computational engine: a very snazzy calculator that you should see as your friend and ally in the journey to understand and appreciate psychology. It sits alongside what we teach about the concepts and interpretation of statistical analysis. R is the core software, RStudio is the interface for interacting with it. Like even a simplest calculator, it just does what you ask (at least when you ask nicely!) but it requires the user to know what they want from it and to understand what it is telling you. A calculator cant help a kid get the right answer to a multiplication problem if they dont know the difference between multiplication and division and addition etc. And whilst a calculator is brilliant at doing the number crunching (and as a bonus, RStudio can help with turning the numbers into beautiful graphs and images too), even a calculator requires a thoughtful person to take the answers and make sensible interpretations from them. Therefore, we need to learn both about the concepts of statistical analysis on the one hand, and the processing of statistical information -through R- on the other. Over the next year, in these labs we will increasingly be using RStudio to focus on the latter, processing side, which will allow you to focus your energies on the conceptual side and its relevance for appreciating psychology. 2.1.2 Getting started with R Studio For Lancaster University Psychology Students, we recommend using the virtual lab installation, through which you create a PC-in-a-web browser and can then use R Studio through AppsAnywhere: https://mylab.lancaster.ac.uk/ Connect to a LU Windows 10 Lab. Once youve logged on to this virtual desktop, you should be presented with the AppsAnywhere window: Search for R Studio and click launch. When RStudio starts, it will look something like this: We also recommend that you set up your University H drive by following the instructions here If you do this, you can organise your files in one place and access them from any university computer that you log in to, including the lab machines in the Levy Lab and the virtual machine (see above). So this effectively acts as a free, cloud-based storage solution for your work (and even personal stuff). 2.1.3 File organisation However you choose to store your files, its essential that you are careful about file organisation. As you progress through these exercises, you will find yourself downloading many data files, and creating many R scripts. Consider each week a new set of materials that should have its own folder. Get used to organising your work and you will be well placed to revise the material at a later date. 2.1.4 Lets do something! In the console window on the left hand side, theres a command prompt &gt;. This is where we ask RStudio to do our bidding! Click in the console window and we will get R to work as a calculator. Type in: 5 + 5 and press enter. You should get the answer (amazing huh? OK, maybe not that amazing). Use your imagination  ask a simple arithmetic question of your own choosing! In the first analysis lecture, we looked at measures of central tendency and how to calculate them. So lets get R to do these calculations also! First, we tell R about the data from the lecture. Copy the following line and paste it into the console, then press enter to run it: lecture1_data &lt;- c(7,8,8,7,3,1,6,9,3,8) This creates an object called lecture1_data. We can then perform calculations on this object. For example, we can find the mean by using the following command (again, copy and paste) mean(lecture1_data) Check the answer is the same we found in the lecture (6!). Next, lets ask for the median: median(lecture1_data) This also should be the answer from the lecture (7) R doesnt have a single corresponding command for the mode, but we can use this series of commands: getmode &lt;- function(lecture1_data) { uniqv &lt;- unique(lecture1_data) uniqv[which.max(tabulate(match(lecture1_data, uniqv)))] } getmode(lecture1_data) This is just a bit of clever jiggery-pokery that gets the mode. 2.1.5 Extra content The commands above are designed to show you that with RStudio active, you can get quick and accurate answers to material covered in PSYC121 lectures. All we have asked is that you write (or copy in) text to get the information. However, after the lab, you could play around with RStudio in your own time and think about the following: In R, &lt;- is the assignment operator as in the command we used: lecture1_data &lt;- c(7,8,8,7,3,1,6,9,3,8) We create the variable label on the left (lecture1.data) and we give it those number on the right. The name lecture1.data is largely arbitrary: try use a variable of your own naming (your own name?) instead - and then use that alternative name for the other commands. What does that tell you about the text used to get the mode? Can you figure out what each line does? Also, once you have created a variable, you can check what the variable comprises by calling it at the command line. Just write its name, and R should respond with all the data points (all the X values it knows about). Take note that, as you write the variable label, R studio should offer to auto-complete the name. It only does that if it you have defined the variable, or are writing a known command. "],["descriptive-statistics-week-2.html", "3 Descriptive statistics (Week 2) 3.1 Pre-lab work 3.2 R Studio tasks", " 3 Descriptive statistics (Week 2) 3.1 Pre-lab work Much of what a psychologist does involves numbers. For example, we: Measure behaviour in terms of numbers Use these numbers to provide summaries that indicate similarities and differences between different conditions (e.g., collections of male and female data used to examine gender differences). Analyse the raw data or the summaries to determine if there are real differences between our experimental conditions. Interpret the final numbers from an analysis in order to extract the underlying psychological differences. It should be obvious from the above that we need detailed information about what numbers mean, how to collect numbers, how to treat numbers and how to analyse numbers. In this lab we will start you along this journey. Lets start by recapping some important concepts from last week: List the 4 types of measurement scales and for each give an example of how they might be used in psychology. For each of the types of numbers below give the scale of measurement, justifying your answer. Temperature in centigrade The number of children who point at a visual stimulus The number of errors made by each participant in a word reading task The rank attractiveness of each of 10 faces The time it takes to process read sentences What is the name given to a bell-shaped distribution? A distribution that is asymmetrical with a long tail that extends towards high numbers is called a ___________________ skewed distribution. A distribution with two peaks is called a ____________________ distribution. 3.2 R Studio tasks For a reminder of how to load R Studio see the section Getting started with R Studio Once you have accessed R Studio (and ideally setup your H drive), download the data file for this week, which is called penelope.csv, and the accompanying R script which we will use later. 3.2.1 How much does the cow weigh? (Task 1) Some 5 years ago, a large group of participants gave an estimate of the weight of Penelope the cow. Just over 17,000 guesses. And the distribution of guesses was something like this: What we can see from this graph is that: Guesses formed a roughly normal distribution. There is a bit of a skew with a right0hand tail, but this is inevitable as a weight of less than 0 is physically impossible, but there is no limit of the semantics of a large guess. The mean guess weight (1,287 lbs) is very close to the actual (true) weight of the cow (1,355 lbs). So even though lots of people were inaccurate, a central tendency measure has a pretty good alignment with te true weight. This is known as the Wisdom of Crowds phenomenon, first identified by Galton in 1907 (though he suggested using the median weight) Lets look at the PSYC121 student data collected on guessing the weight of Penelope, and ask whether it resembles the properties of this large dataset. 3.2.2 Finding the correct folder we need something here about navigating to the folder and setting the working directory 3.2.3 Using the R script Lets start working with our data, by opening up the PSYC121_week_2_script.R file. This is an R script that contains a series of commands. We could type each of these in the console window (like we did in Week 1), but by having them in a script like this, it saves us typing them or copying and pasting them into the console. It also means we have a record of all the commands we have run. The first command is loads a library of functions: library(tidyverse) To run this, simply click anywhere on line 1 to put the cursor there, and press ctrl+enter (cmd+enter on a mac) or click the button called run. You will see a number of messages appear in the console. Dont worry about these, or worry too much about what exactly this command is doing. Essentially this is giving us some useful tools for our analysis. We will introduce the features of the tidyverse gradually during this course. Lets now get our weight guessing data into RStudio. After we have loaded the tidyverse library, we can then use the read_csv() command to read in the dataset: penelope &lt;- read_csv(&quot;penelope.csv&quot;) Have a quick look at the data you have just imported / loaded. To do so, at the command line just write penelope and the data will be reported. Note that NA means not available or missing data. Does this file structure make some sense to you? 3.2.4 Finding the mean and median estimates Use the data to answer the following questions What is the mean weight estimates? What is the standard deviation of the estimates? What range of estimates encompasses 2 standard deviations around the mean? What is the median weight of the estimates? Which of these central tendency measures is the more accurate measure of the true cow weight? What is the mean weight estimate (and standard deviation) for female respondents and non-female (male and non-binary) respondents? You may be thinking, how do I possibly do any of this?! Well this week all the commands you need are contained in the R script you have downloaded. Also, remember from last week, we explored the R command: mean(lecture1_data) That gave us the mean of the small dataset lecture1_data. This time, we want to explore the Penelope dataset. But also, the lecture_data was just a single list of numbers. The Penelope file is more like a datasheet. So we need to tell RStudio which column we are interested in. RStudio uses the format data$column. Hence, we can ask mean(penelope$estimate) and to get a standard deviation we can use the command: sd(penelope$estimate) So from this, can you work out what you would do to get the median value (remember from last week how we got the median value?). Add this command to your script, run it to get the median value, and save the file. 3.2.5 Referencing columns We have seen that: mean(penelope$estimate) will provide a mean of the column estimate. In the third column, named fweight, we have the estimates of just the female respondents. In the fourth column, named oweight, we have the estimates of the other respondents (males and non-binary). So can you now figure out how you might get information about the other columns, such as the female data (only) or the non-female data? Try it, based on what you have just done. Does it work? You will find that the result of the this command produces an NA result. This means that the answer is Not Available, or in other words, is a missing value. This is because some of the values in this column are NA, and the mean of a column with NAs will always lead to the result NA. Instead, try this command: mean(penelope$fweight, na.rm = TRUE ) Any different? The na.rm = TRUE instruction tells RStudio that missing data can be ignored in this mean calculation. (in technical language, na.rm is a parameter of the function mean that removes the NAs if set to TRUE) 3.2.6 evaluating the range of estimates mean +/- 2 SDs exercise JOHN - this probably needs a bit more intro here - asking a bit too much at this stage? Q3 and Q5 above ask you to think about and process what R Studio tells you from the other commands you are using. The information comes from elsewhere in the activities you have done 3.2.7 simple graphs RStudio can be used to create graphical data plots that can help interpret datasets The first thing we can do is create a histogram distribution of guesses from the sample student data to compare with the previous large sample study (i.e. the 17,000 guesses): hist(penelope$estimate) We can also create a box and whisker plot. Heres a general simple description of a box-and-whisker plot as a graphical representation of data: We can create a box and whisker plot for the estimate column using the following command: boxplot(penelope$estimate) "],["week6.html", "4 Grouping and summarising (Week 6) 4.1 Pre-lab work 4.2 Lab Work", " 4 Grouping and summarising (Week 6) In this weeks lab we will be recapping some of the commands you learnt in week 3 - group_by() and summarise() - and exploring some of the properties of the binomial distribution using the Sign Test. Please ensure you do the pre-lab work before coming to class. 4.1 Pre-lab work 4.1.1 Online tutorials Complete the online tutorial for working with these commands. These tutorials use a web-based version of R that will allow you to enter commands, and also get hints and solutions to the problems. Once youve completed the tutorial, you can put what youve learnt in to practice in R Studio using the lab tasks below. Click here to access the online tutorial for this week 4.1.2 R Studio prep work In preparation for the lab tasks, please do the following things: Download the files location_music_Wk6.csv, Week_6_lab.R and Week_6_binomial.R. Place these files in a suitable folder on the computer. Within R Studio, navigate to that folder in the Files pane. Select the More button and then Set as Working Directory Recommended - create a new project in this folder (File, New Project, Existing Directory) 4.1.3 Conceptual prep work A researcher conducts an experiment to find out if an anti-smoking advert changes the amount people smoke. He has people record the number of cigarettes they smoke for a week before watching the advert, and then for a week after watching the advert. The data are shown below: Participant Before watching the advert After watching the advert 1 25 31 2 10 5 3 8 7 4 35 25 5 48 44 6 21 26 7 29 30 8 17 11 9 26 14 10 41 38 11 44 12 12 73 56 13 4 0 14 8 8 15 33 24 16 22 27 How many positive, negative, and zero differences are there? What is the sample size that is applicable for the Sign Test? What is the critical value in the Sign Test table? What is the null-hypothesis? Can we reject the null-hypothesis? 4.2 Lab Work 4.2.1 R Studio exercises - music preferences (Task 1) Now that you have completed the online tutorials on group_by() and summarise(), its time to put those skills into practice with a new dataset. Open the Week_6_lab.R script (as detailed in the pre-lab work). This will load the tidyverse packages and read in the data. The data consist of the ratings you gave for 8 different musicians, from a score of 1 (Dont like them) through to 5 (Love them!). People could give no opinion, which are coded as NA. So be sure to use na.rm = TRUE when computing your summary statistics. Follow these steps to explore this dataset: Run the two lines of codes library(tidyverse) and data &lt;- read_csv(\"location_music.csv\") Use summary() to get an overview of the data View the responses to nominal variables with count() For an artist of your choice, calculate the mean and SD of their ratings. Use group_by() and summarise() to see whether peoples home location changes their ratings for an artist of your choice. 4.2.2 Exploring binomial distributions and the sign test (Task 2). Download the Week_6_binomial.R script from the Moodle forum. This is a version of the binomial program that Tom used in his lecture this week. It allows you to set the size of the sample e, and generate the binomial probabilities of different results for that sample size. It then plots those probabilities as a column graph using geom_col(). The red line shows the significance level and allows you to see which events are unusual given the null hypothesis for the given sample size (i.e., which bars are below this line) and for a specified Type 1 error rate (alpha). The probabilities for the different results are also presented in blue. Would a result of 3 positive and and 13 negative be deemed statistically significant at the 5% level? Use the script to generate this result and compare the graph to the appropriate number in the Sign Test table below. Would the result be significant at the 1% level? What does this tell you about setting the Type 1 error rate at 1%? With a sample size of 12, use the Sign Test table below to note how many positive or negative results would be deemed unusual under the null hypothesis, at the 5%? Run a sample of 12 in the script to check and compare your answer to 4. For a sample of 8, what significance level (Type 1 error rate; alpha) would we need to set for a result of 2 out of 8 to be deemed statistically significant? Why might this be a poor criterion for our Type 1 error rate? 4.2.3 The Sign Test table "],["visualisation-i-week-7.html", "5 Visualisation I (Week 7) 5.1 Pre-lab work 5.2 Lab Work (R Studio)", " 5 Visualisation I (Week 7) They say a picture paints a thousand words, so in this weeks lab we will be learning some fundamental skills in data visualisation with the ggplot() commands. We will also be looking at how we can run one-sample t-tests in R and we will use these to explore the estimates people gave for the median salary in the UK. Please ensure you do the pre-lab work before coming to class. 5.1 Pre-lab work 5.1.1 Online tutorial Click here to access the online tutorial for this week 5.1.2 R Studio prep work In preparation for the lab tasks, please do the following things: Download the files data_phone_Wk7.csv, data_salary_Wk7.csv, Week_7_Task_2.R and Week_7_Task_3.R. Place the files in a suitable folder on the computer. Within R Studio, navigate to that folder in the Files pane and click OPEN (note the files are not shown in the folder) Select the More button and then Set as Working Directory Optional - create a new project in this folder (File, New Project, Existing Directory) 5.1.3 Conceptual prep work A researcher conducts an experiment in which they ask a group of ten children aged 8 to read as many words as they can in 1 minute. They know from previous studies that children of this age typically read at 120 words per minute. This group of children have been reported as having difficulty with reading and the researchers want to see if their data are unusual. They find that the 10 children show a reading speed of 105, with a SD of 18. What is the expected mean under the null hypothesis? What is the difference between the sample mean and the expected mean (mu)? What is the standard error of the mean? What is the value of t? What is the value for the degrees of freedom? What is the criterion value in the t-test table (see below), at the 5% level? Can we reject the null-hypothesis? 5.2 Lab Work (R Studio) 5.2.1 Visualising phone use Now that you have completed the online tutorials on ggplot(), its time to put those skills into practice with a new data set, provided in data_phone.csv. These data are just the phone time variables (estimated and actual) with the phone type nominal variable. Follow these steps to visualise these data: Open the script Week_7_Task_2.R (see prep work) Run the first two lines of code: library(tidyverse) data &lt;- read_csv(&quot;data_phone.csv&quot;) View the data with view(data) Inside the aes() command, map the variable screen_time_estimate to the x axis, and the variable screen_time_actual to the y axis. In general, does it look like peoples estimates were accurate? Add a setting for colour outside of the aes(), to make all the points red. Map the colour to the variable phone_type (within aes()). Can you see any differences between people who have different phones? Change the axis titles and add a title for the graph using the labs() command. Change the theme of the graph. 5.2.2 Conducting a one-sample t-test. In this task we will run some one-sample t-tests in R to examine peoples estimates of UK salaries. Open the Week_7_Task_3.R script (see prep work) Run the first two lines of code: library(tidyverse) data &lt;- read_csv(&quot;data_salary.csv&quot;) Run the third command. This runs a t-test on the salary estimates. The two settings (parameters) in the t-test function are x, which is the column of data we are interested in testing, and mu, which is the value we want to compare against. Look at the output of the t-test. What does the t value indicate? Check the value against the criterion value in the t-test below (our N = 205, so use value for infinity). The p value tells you precisely how likely the data are, if the null hypothesis was true. Would we say this is statistically significant? Use the 4th command to filter the data to just those participants whose home location is the UK. Run the t-test on these data (you can copy the command, but note you will need to change the name of the data) In the same way as for Q6, check the salary ratings for those people whose home location is Asia and Europe (not UK). Note in each case what the t-test is telling you. The t-test table (criterion values of t) "],["visualisation-ii-week-8.html", "6 Visualisation II (Week 8) 6.1 Pre-lab work 6.2 Lab Work (R Studio)", " 6 Visualisation II (Week 8) Today we will continue to develop our skills in data visualisation with the ggplot() commands. We will look at plotting means and standard errors, and at how geoms can share aesthetic mappings. We will explore our data on the famous Stroop Task and calculate 6.1 Pre-lab work 6.1.1 Online tutorial Click here to access the online tutorial for this week 6.1.2 R Studio prep work In preparation for the lab tasks, please do the following things: Download the files data_stroop_Wk8.csv, and Week_8_Tasks.R Place the files in a suitable folder on the computer. Within R Studio, navigate to that folder in the Files pane and click OPEN (note the files are not shown in the folder) Select the More button and then Set as Working Directory Optional - create a new project in this folder (File, New Project, Existing Directory) 6.1.3 Conceptual prep work A researcher wants to replicate a published finding that submerging your arm into freezing cold water for 60 seconds impaires your ability to memorise lists of words. They have 15 people complete the memory test before and after submerging their arm. The mean score before is 17 items correct, and the mean score after is 12 items correct. The SD of the difference scores is 9. What is the null hypothesis? What is the mean difference score? What is the SE of the difference scores? What is the t value? What are the degrees of freedom? What is the criterion value for t? (see table below) Is our t value significant? Is this a directional hypothesis? And what does that mean for our criterion value? 6.2 Lab Work (R Studio) 6.2.1 Exploring the stroop task The Stroop Effect is a classic demonstration of automaticity of behaviour. Participants have to say the colour a word is printed in, which is an easy task for a compatible stimulus like , and a much more difficult task for an incompatible stimulus like . We cant help but read the text - it has seemingly become an automatic process. In this task we will calculate the means and standard errors of the means. We will then plot them using ggplot(). Open the script Week_8_Tasks.R (see prep work) Run the first two lines of code: library(tidyverse) data &lt;- read_csv(&quot;data_stroop.csv&quot;) View the data with view(data). You will see that the data are a little different from the data we have worked with previously. We have an ID variable, which gives a unique number for each person. Each person has 3 rows. This is because the different conditions of the stroop task is a within-subjects variable. For data like this it is often useful to have them arranged in what is referred to as long format, with multiple rows for each response the participant provided. For the current data that means we have a variable called stroop_condition, which is our IV, and one called stroop_time which is our DV. 6.2.2 Analysing the data Return to the Week_8_Tasks.R script. You will see we have provided you with the scaffold of a complete analysis process for the Stroop data. The raw data will be grouped and then summarised as means and SEs, and then plotted with columns and error bars. This is very similar to the commands used at the end of the learnr tutorial, and you can refer to that analysis if you get stuck here. Your job is to carefully work through the code and fill in the parts that are marked MISSING. By the end, you should have a graph with 3 columns, with the standard error of the means plotted on top. Add a labs() layer to the plot to change the axis titles, and the title of the plot. Change the theme of the plot Map the fill aesthetic to the variable stroop_condition Manually change the colours of the columns with (e.g.) scale_fill_manual(values = c(\"darkgreen\", \"darkblue\", \"darkred\")). Open the RColor.pdf in the Week 8 files for more colours than youll probably ever want! Click Export -&gt; Save as Image. 6.2.3 Testing if the differences are real. To test whether people performed differently in the different Stroop conditions, we can run a series of related samples t-tests. We will use a slightly modified version of our command for conducting the one-sample t-test in Week 7. First though, we must use a filter() to restrict the data to just two conditions on the stroop_condition variable. This is because a related samples t-test looks at the difference between two means (and only two), so the column we use needs to have just two levels. Continue working with the Week_8_Tasks.R script Use the filter command to restrict the data to two of the conditions. Note that the filter uses an | symbol, which means or. Run the t-test on this selection of data, to compare the means from these two levels of the IV. Is the result significant? Note the t value and the p value. How would you express this as a statement in a report? With 3 levels to the variable stroop_condition there are 3 possible comparisons we can make. Complete these by copying and pasting the commands, editing each to make a different filter selection, and then run the t-test. Make sure you interpret the results of the t-tests. The t-test table (criterion values of t) "],["filtering-outlier-values-week-9.html", "7 Filtering outlier values (Week 9) 7.1 Pre-lab work 7.2 Lab Work (R Studio)", " 7 Filtering outlier values (Week 9) Today we will do some work on checking data for unusual values, determining what is an outlier and removing those data before our analysis. In order to achieve this important step, we will learn a bit more about filtering data using the filter() command, and learn a new function, mutate(), which creates new columns. 7.1 Pre-lab work 7.1.1 Online tutorial Click here to access the online tutorial for this week 7.1.2 R Studio prep work In preparation for Task 2, please do the following things: Download the files gamble_salary_Wk9.csv, and Week_9_Tasks.R Place the files in a suitable folder on the computer. Within R Studio, navigate to that folder in the Files pane and click OPEN (note the files are not shown in the folder) Select the More button and then Set as Working Directory Optional - create a new project in this folder (File, New Project, Existing Directory) 7.1.3 Conceptual prep work This work gets you to play with the power calculations in R, as shown in the final slide of the lecture this week. Start a new R Studio script and complete the following queries: Add a line library(pwr) and run this add the line pwr.t.test(d = .5, n = 100) and run this. What is the reported power value and what does this mean? If you want to achieve 99% power to see an effect of .3, what sample size do you need? With a sample size of 60 and power of 80%, what minimum effect size would we be able to detect? 7.2 Lab Work (R Studio) 7.2.1 Exploring the median salary estimates In this task we will look at the data for UK median salary estimates, and focus first on exploring the sample in order to identify and potentially remove any unusual data. Open the script Week_9_Tasks.R (see prep work) Run the first two lines of code: library(tidyverse) data &lt;- read_csv(&quot;gamble_salary.csv&quot;) Add a geom_boxplot() to the ggplot() command to draw a box and whiskers plot, mapping the variable salary_estimate to either the x OR y axis. Note the outliers that are shown - we will now try to identify these data points and remove them. 7.2.2 Using mutate to create a new column Add a new column (mutate()) to the data set, which uses the scale function to transform the salary estimates into z-scores. Complete the arrange(desc()) to have the data sorted by the z-score column (use whatever name you made that column in the mutate command). Use view(data) to view the data set and get a sense of the data. Now that we have the salary estimates recoded as z-scores, we can think about what we might consider an extreme data point. If we remove z scores that are &gt;2 and &lt;-2 we will be removing ~5% of the extreme values. ### Filtering the data Lets take a look at how many results have these extreme z values in our sample. Complete the two filter commands, one to see how many results give z-scores over 2, and one to see how many results give z-scores under -2. You should see 6 at the high end, and 11 at the low end. Once you are happy with how the filters are working, use a filter command to remove these outliers from the data. 7.2.3 Salary estimates and risk-seeking We will look at the salary estimates, as a function of whether respondants were risk-seeking or more risk-averse. You may remember that you were asked the following question: Imagine you are on a game-show and you win £300. The presenter gives you have a choice: receive an additional £100 for sure, or take a gamble offering a 50% chance to gain £200 and a 50% chance to gain nothing. Which would you choose? The classic finding is that around 80% of people play it safe and dont gamble. Whether people in our sample chose to gamble or not, is coded in the column gamble_gain (TRUE = gamble, FALSE = dont gamble). Complete the group_by and summarise commands to assess whether the mean salary estimates differed between people who were risk-takers and those who were more risk-averse. Note that the summarise command includes a calculation of N, showing the number of each response: did our sample fit the original finding of ~80% people playing it safe? 7.2.4 Unrelated samples t-test Run a unrelated (paired = FALSE) t-test on the data to see if people who are more risk averse or risk-seeking give different salary estimates. For those interested in the gambling variable, note that we asked a second version of the same scenario: Imagine you are on a different game-show and you win £500. Here you are given a choice between either losing £100 from your winnings for sure, or taking a gamble offering a 50% chance to lose nothing and a 50% chance to lose £200. Which would you choose? When phrased as a loss, more people tend to gamble. Did our sample show this result? Were the salary estimates of people who were willing to gamble under this scenario different to those not willing to gamble? Use the same formula you used in the t-statistic in the calculation of Cohens d. What is the effect size? Work out what our final sample size was in the data after we conducted the filtering (hint: look in the environment). Use this to conduct a power analysis to see what effect size we could have seen with a power of 80%. The t-test table (criterion values of t) "],["correlation-week-11.html", "8 Correlation (Week 11) 8.1 Pre-lab work 8.2 Lab activities", " 8 Correlation (Week 11) 8.1 Pre-lab work 8.1.1 Online tutorial 8.1.2 R Studio prep work Download the file MillerHadenData.csv, and 122_wk11_labActivity.R 8.1.3 Conceptual prep work Click here for a basic web app that will test how good you are at recognising different correlation strengths from the scatterplots. We would recommend you click the Track Performance tab so you can keep an overview of your overall bias to underestimate or overestimate a correlation. Is this all just a bit of fun? Well, yes, because stats is actually fun! But also no, because it serves a purpose of helping you determine if the correlations you see in your own data are real, and to help you see if correlations in published research match with what you are being told. As you will have seen from the above examples, one data point can lead to a misleading relationship and even what might be considered a medium to strong relationship may actually have only limited relevance in the real world. 8.2 Lab activities In this lab, youll gain understanding of and practice with:  constructing and interpreting scatterplots  running correlation analysis and interpret the results  reporting the results in APA format  constructing a correlation matrix in APA format  when and why to apply correlation analysis to answers questions in psychological science 8.2.1 Interpreting correlation Below are scatterplots that show the relationship between how much you know about correlation and how attractive you appear to members of the opposite (&amp;/or same) sex. Choose the type of correlation (strength and direction) displayed in each graph using one of the following:  Perfect positive correlation  Perfect negative correlation  Strong positive correlation  Strong negative correlation  Moderate positive correlation  Moderate negative correlation  Null correlation Suppose it was observed that there is a correlation of r = -.81 between a drivers age and the cost of car insurance. This correlation would mean that, in general, older people pay more for car insurance. TRUE or FALSE? Explain why: Note: explain your chosen answer based on the statistic given, not why you think the correlation may or may not make logical sense. 8.2.2 Conducting an analysis of correlation in R We will now conduct a correlational analysis of some data. You can follow along with this analysis by downloading the data and R script (see above). Load the necessary packages and read in the data: library(&quot;broom&quot;) library(&quot;tidyverse&quot;) mh &lt;- read_csv(&quot;MillerHadenData.csv&quot;) mh # view the data in the console We can use the ggplot with the geom_point feature to draw a scatterplot # plot the relationship between home and TV using a scatterplot and a line of best fit ggplot(mh, aes(x = Home, y = TV)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_bw() + labs(x = &quot;Time spend reading at home&quot;, y = &quot;Time spend watching TV at home&quot;) This gives a visual representation, but we can assess the correlation statistically using the cor.test() function: # conduct a correlation analysis, using Pearson&#39;s r # save this as &quot;results&quot; results &lt;- cor.test(x = mh$Home, y = mh$TV, method = &quot;pearson&quot;, alternative = &quot;two.sided&quot;) %&gt;% tidy() # from the broom package - gives a nicer output for the correlation result results # print the results The results object contains details about the correlation test. We can use the pull() function to extract the different statistical information: # pull out Pearson&#39;s r, the degrees of freedom and the p-value for reporting the results r &lt;- results %&gt;% pull(estimate) %&gt;% round(2) df &lt;- results %&gt;% pull(parameter) pvalue &lt;- results %&gt;% pull(p.value) %&gt;% round(3) rsquared &lt;- r*r rsquaredPercent &lt;- round(rsquared * 100, 0) "]]
